---
title: "Midterm_Koob_Elise_2018"
author: "Elise Koob"
date: "November 9, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Midterm Exam

### 1) Sampling your system (10 points)
#### **Each of you has a study system you work in and a question of interest. Give an example of one variable that you would sample in order to get a sense of its variation in nature. Describe, in detail, how you would sample for the population of that variable in order to understand its distribution.** 

My research focuses on black sea bass, a demersal marine fish becoming increasingly more common in Massachusetts waters. One variable of interest that is measured for all commercially and recreationally harvested species is a length distribution of the population. When paired with an age-length key, many important population parameters can be estimated from this information, including growth rates, age/size at maturity, longevity, etc. (Campana, 2001). These parameters are used in stock assessments that help manage fisheries. Therefore, the length distribution of a population can be extremely important information.

Black sea bass have expanded their population distribution farther north than historically reported, increasing in numbers throughout Massachusetts (NEFSC, 2017). Recent inquiries have arisen about spawning activity and distribution north of Cape Cod (McBride et al, 2018), where there have been no reported occurrences. The following sampling design is based on if a study were conducted to collect information on the length distribution of spawning adult black sea bass in Massachusetts waters north of Cape Cod. The Massachusetts Division of Marine Fisheries Resource Assessment Trawl Survey is limited by bottom type, therefore, a study such as this may need to be implemented to address this question. 


#### **Just what is your sample versus your population?** 

The sample would be the fish captured for which data was collected whereas the population would be all spawning black sea bass in Massachusetts waters north of Cape Cod.

#### **What would your sampling design be? Why would you design it that particular way?** 

I would conduct a rod and reel survey to capture black sea bass during the spawning season. This type of survey design would allow for sites to be selected on a variety of bottom types, expanding the area in which data can be collected than what is available from the Trawl Survey. Black sea bass tend to congregate around rock piles and wrecks, which cannot be surveyed using a trawl net. Sites throughout Cape Cod Bay, Massachusetts Bay and along the North Shore would be chosen and sorted based on bottom type and depth. This would reduce the amount of bias created if only sampling one type for each. Sampling of 30 randomized sites would occur once a month (new set of randomized sites each month) beginning in June and ending in September, the duration of black sea bass spawning at the northern edge of their range. Randomization is important so that no bias is instilled on the selection of sampling areas. Additionally, I believe 30 sites would be a reasonable number that would keep costs as low as possible while still allowing for adequate sampling of the area north of Cape Cod. Four anglers would fish with a rod and reel at each site for 1 hour with the same gear and bait as it would be important to keep fishing effort consistent among all sites throughout the study.

Specimens successfully brought onboard would be measured, scale sample taken (for age determinations) and gently squeezed for sex and maturity determinations. [Mature, spawning condition fish (usually classified as 'ripe' or 'ripe and running') will release eggs or sperm when the abdomen is pushed lightly.]

#### **What are potential confounding influences of both sampling technique and sample design that you need to be careful to avoid?** 

Black sea bass are noted as opportunistic feeders (Kendall, 1977) but if there was a preference then catch may be impacted. However, since the bait will be kept consistent throughout the study, this impact should be consistent across all sites and not create a bias in the study. This study is also dependent on black sea bass actively feeding during spawning season. I am unaware of any studies looking at the feeding behavior of black sea bass during spawning season but any decrease in appetite would also impact catch rate. However, again, this should be consistent across sites and the study and not create a bias in data collected.

Angler experience has the potential to cause bias in the study. Due to this, a mix of experience levels should be used in each trip as more experienced fishers are likely to have a higher catch rate than more inexperienced anglers. Surveys can be completed to assess fishing experience prior to fishing trips. 
Sampling trips within each month should occur around the same time every month and avoid uneven temporal spacing of sampling. For example, sampling at the end of June, then immediately at the beginning of July and then waiting until late August. 

Overall, consistency in sampling technique and design between sites and across the entire study should be a priority.

#### **What statistical distribution might the variable take, and why?** 

Black sea bass length distribution would likely a normally distributed frequency (bell curve). This is due to high mortality in younger and older fish in a population and therefore a peak in the mid-range of lengths/ages. 


##### **Works Cited**  
> ###### Campana, S. E. (2001). Accuracy, precision and quality control in age determination, including a review of the use and abuse of age validation methods. Journal of Fish Biology, 59, 197-242. 
> ###### Kendall, A. W. (1977). Biological and Fisheries Data on Black Sea Bass, Centropristis striata (Linnaeus). Highlands, NJ.
> ###### McBride, RS, MK Tweedie, and K Oliveira. 2018. Reproduction, first-year growth, and expansion of spawning and nursery grounds of black sea bass (Centropristis striata) into a warming Gulf of Maine. Fishery Bulletin 116: 323-336.
> ###### NEFSC. (2017). 62nd Northeast Regional Stock Assessment Workshop. Assessment Summary Report 17-01.    


###2) Let's get philosophical. (10 points) 
#### Are you a frequentist, likelihoodist, or Bayesian? Why? Include in your answer why you prefer the inferential tools (e.g. confidence intervals, test statistics, posterior probabilities, etc.) of your chosen worldview and why you do not like the ones of the other one. This includes defining just what those different tools mean! 

#### Extra credit for citing and discussing outside sources - one point per source/point

A frequentist approach to statistics involves testing data collected from an experiment or observational study to a specific null hypothesis. This testing will lead to either rejecting or not rejecting the null hypothesis in favor of an alternate hypothesis. Mayo & Cox (2006) discusses theory behind frequentist inference as that it is based on repeated experiments, i.e. an acquired result one would get if the experiment or study were repeated an infinite number of times. This approach uses test statistics to examine the conformity of the study data to the null hypothesis, followed by p-values to give the probability of getting that result, or a result more extreme, when the null hypothesis is true. A significance level is applied (often 0.05 or 0.01) for the p-value as threshold, below which the null hypothesis would be rejected and the data regarded as 'statistically significant'. Confidence intervals are also used (usually 95% confidence intervals) to give a range of values in which one can be 95% confident the true value of that parameter lies within.

One of the biggest debates surrounding frequentist inference is the use of p-values as the threshold for rejecting null hypotheses(Mayo & Cox, 2006). It is up to the researcher to set the alpha value for the p-value which adds subjectivity to a process that should be objective. Additionally, many times the p-value is misinterpreted by relating statistical significance to study result significance. The American Statistical Association discusses this issue in full, clarifying that a smaller p-value does not equate to a 'more significant' finding (Wasserstein & Lazar, 2016). 

Overall, frequentist inference seems to be the most wide-spread statistical analysis used thus far but is also surrounded by debate regarding p-values and the underlying theory of testing data given a specific hypothesis (p(D|H)). The latter seems very unintuitive to me. Additionally, it seems bizarre and backwards that we would test a null hypothesis with the goal of rejecting it in order to support our actual hypothesis versus being able to test our actual hypothesis in the first place. Due to these reasons, I do not believe that I am a frequentist when it comes to data analysis. 

Likelihood inference is based off of frequentist inference and uses model comparisons to identify how well data supports a hypothesis. The Likelihood Function (often the Log Likelihood Function) and Maximum Log Likelihood are used to find values for model parameters that have the highest likelihood given a specific dataset. Confidence intervals are used (like in frequentist inference) to define the range of values in which there is 95% confidence the true value lies within. Again, this method of analysis is testing how well the data collected fits a certain hypothesis and not the other way around. 

Upon further reading, the Law of Likelihood uses a likelihood ratio to compare one hypothesis to another and the hypothesis with the higher likelihood is supported. Blume (2002) discusses one advantage of this method as not being sensitive to the same things as p-values are (such as the number of examinations). However, overall this method still seems problematic to me (if used exclusively) given that one would have to compare every possible hypothesis in order to arrive at one 'true' hypothesis. It seems improbable, or even impossible, that one could consider every hypothesis. It is for these reasons that I may not be a likelihoodist in full. 

Bayesian inference incorporates likelihood as part of a larger equation that uses prior knowledge to inform the model being used and give a probability distribution of values. This method uses Bayes' Theorem of probability to assign a degree of belief to parameter values. A posterior density distribution results from multiplying a prior probability by a likelihood function and normalizing by the marginal likelihood (Brewer, 2012). Additionally, credible intervals are calculated to show where 95% of the probability mass from the posterior distribution is located. This, unlike confidence intervals, gives a range of probabilities where a parameter is located within a region.

The largest difference between Bayesian and Likelihood/Frequentist approaches is the incorporation of prior knowledge about the system in question from previous studies or basic understanding. Bernardo (2003) describes this as a major advantage to Bayesian Inference along with being amendable to complex analyses. In addition, it allows for comparison of hypotheses by looking at probability. Unlike the two other methods, Bayesian inference evaluates a hypothesis based on the data which seems to me the more logical progression of data analysis. Although incorporation of priors is debated, I think that it is beneficial to include them. If no previous studies have been done in which to draw parameter values for the prior, then a flat prior can be used (Bernardo, 2003).

From what I know thus far about these three inferential techniques, I think that I am the most Bayesian out of them. It seems as though including prior information about the system you are working with would be the most intuitive way to complete data analysis and receive informed results. Additionally, it is the method in which we are able to test a hypothesis based on the data collected (p(H|D)) and not vice versa. 

##### **Works Cited**  
> ###### Bernardo, J. M. (2003). Bayesian statistics. In Probability and Statistics (Vol. R. Viertl, pp. 1-46). Oxford, UK: UNESCO,: Encyclopedia of Life Support Systems (EOLSS).
> ###### Blume, J. D. (2002). Likelihood methods for measuring statistical evidence. Statistics in Medicine, 21, 2563-2599. 
> ###### Brewer, B. J. (2012). Introduction to Bayesian Statistics. Creative Commons Attribution-ShareAlike 3.0.  1-97.
> ###### Mayo, D. G., & Cox, D. R. (2006). Frequentist statistics as a theory of inductive inference. In IMS Lecture Notes-Monograph Series 2nd Lehmann Symposium - Optimality (Vol. 49, pp. 77-97). Institute of Mathematical Statistics.
> ###### Wasserstein, R. L., & Lazar, N. A. (2016). The ASA's Statement on p-Values: Context , Process , and Purpose. The American Statistician, 70(2), 129-133. 


### 3) Power (20 points) 

```{r, warning = FALSE, message = FALSE}
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(broom)
library(MASS)
library(profileModel)
library(brms)
library(bayesplot)
library(tidybayes)
library(rstanarm)
library(purrr)
library(beyonce)
```

#### We have a lot of aspects of the sample of data that we collect which can alter the power of our linear regressions. 
#### 1. Slope 
#### 2. Intercept 
#### 3. Residual variance 
#### 4. Sample Size 
#### 5. Range of X values

####Choose three of the above properties and demonstrate how they alter power of an F-test from a linear regression using at least three different alpha levels. As a baseline of the parameters, let's use the information from the seal data: slope = 0.00237, intercept=115.767, sigma = 5.6805, range of seal ages = 958 to 8353, or, if you prefer, seal ages  N(3730.246, 1293.485). Your call what distribution to use for seal age simulation. 


```{r, message=FALSE, warning = FALSE}
#Choosing to vary intercept, resid and sample size

seal_df <- data.frame(slope = 0.00237) %>%
  crossing(intercept = c(110, 115, 120)) %>%
  crossing(resid = c(3,4,5,6,7)) %>%
  crossing(samp_size = c(5:15))

#replicate and add 100 simulations
seal_df <- seal_df %>%
  group_by(slope, intercept, resid, samp_size) %>%   
  expand(reps = 1:samp_size) %>%   
  ungroup() %>%
  crossing(sim= 1:100)

#simulate age 
seal_df <- seal_df %>%
  mutate(age = runif(n(), 958, 8353))

#predict length using y = mx + b  
seal_df <- seal_df %>%
  mutate(length = rnorm(n(), slope*age + intercept, resid))
head(seal_df)

#check to see if data looks ok; YAAAAAS *cough* 5th times a charm...
ggplot(seal_df, aes(x = age, y = length)) +
  geom_point()


#moving on...

```

```{r}
#fit linear model
colnames(seal_df)

#fit linear model and extract coefficients
seal_df_fit <- seal_df %>%
  group_by(slope, intercept, resid, samp_size, sim) %>%
  nest() %>%
  mutate(lin_mod = map(data, ~lm(length ~ age, data = .))) %>%
   mutate(coefficients = map(lin_mod, ~tidy(.))) %>%   
  unnest(coefficients) %>%   
  ungroup()

#add multiple alphas
seal_df_fit <- seal_df_fit %>%
  crossing(alpha = c(0.01, 0.05, 0.10))

#calculate power for intercept
  # power (1 - proportion of wrong p-values at a prespecified )
    # i.e. 1-(# of p > alpha)/nsims

power_int <- seal_df_fit %>%    
  group_by(intercept, alpha)%>%     
  summarise(power=1-sum(p.value>alpha)/n()) %>%    
  ungroup()
power_int

#plot intercept vs power
ggplot(power_int, aes(x = intercept, y = power, color = factor(alpha))) +
  geom_point(size = 4) +
  geom_line(size =2) +
  theme_bw() +
  scale_color_manual(values = beyonce_palette(129))

#just trying another visualization...
ggplot(power_int, aes(x = intercept, y = power, color = factor(alpha))) +
  geom_point(size = 3) +
  geom_line(size = 1.5) +
  theme_bw() +
  facet_wrap(~alpha)

#INTERCEPT NOT MUCH OF AN EFFECT ON POWER


#calculate power for resid
power_resid <- seal_df_fit %>%    
  group_by(resid, alpha)%>%     
  summarise(power=1-sum(p.value>alpha)/n()) %>%    
  ungroup()
head(power_resid)

#plot resid vs power
ggplot(power_resid, aes(x = resid, y = power, color = factor(alpha))) +
  geom_point(size = 4) +
  geom_line(size =2) +
  theme_bw() +
  scale_color_manual(values = c("red", "darkblue", "purple"))

#LARGE EFFECT ON POWER (increasing resid = decreasing power)

#calculate power for samp_size
power_samp_size <- seal_df_fit %>%    
  group_by(samp_size, alpha)%>%     
  summarise(power=1-sum(p.value>alpha)/n()) %>%    
  ungroup()
head(power_samp_size)

#plot sample size vs power
ggplot(power_samp_size, aes(x = samp_size, y = power, color = factor(alpha))) +
  geom_point(size = 4) +
  geom_line(size =2) +
  theme_bw() +
  scale_color_brewer(palette = "PuRd")

#LARGE EFFECT ON POWER-increasing sample size increases power; also looks like it has a larger effect on the smallest alpha level (steeper slope for 0.01 than for 0.05 and 0.10)

```


###4) Bayes Theorem 
#### I've referenced the following figure a few times. I'd like you to demonstrate your understanding of Bayes Theorem by hand showing what the probability of the sun exploding is given the data. Assume that your prior probability that the sun explodes is p(Sun Explodes) = 0.0001. The rest of the information you need is in the cartoon!

The probability sun exploding given data is 0.00348814.

```{r}
#Bayes:
#p(H|D) = (p(D|H)*p(H))/p(D)
  #p(D) = (p(D|H1)*p(H1)) + (p(D|H2)*p(H2))
          #probability of truth*probability of nova +
          #probability of lie*probability of no nova

#p_HD = probability sun exploding given data 
  
p_DH <- 35/36 #probability truth about nova 
p_DH2 <- 1/36 #probability lie about nova
p_H <- 0.0001 #probability of nova
p_H2 <- 1-0.0001 #probability of no nova

p_HD <- (p_DH*p_H)/((p_DH*p_H) + (p_DH2*p_H2))
p_HD

```


### 5) Quailing at the Prospect of Linear Models 
#### I'd like us to walk through the three different 'engines' that we have learned about to fit linear models. To motivate this, we'll look at Burness et al.'s 2012 study "Post-hatch heat warms adult beaks: irreversible physiological plasticity in Japanese quail. We'll be looking at the morphology data. 

###5.1 Three fits (10 points)
#### To begin with, I'd like you to fit the relationship that describes how Tarsus (leg) length predicts upper beak (Culmen) length. Fit this relationship using least squares, likelihood, and Bayesian techniques. For each fit, demonstrate that the necessary assumptions have been met. Note, functions used to fit with likelihood and Bayes may or may not behave well when fed NAs. So look out for those errors. 

```{r, message = FALSE, warning = FALSE}

#load data
quail <- read.csv("../Data/Morphology data.csv")
str(quail)

#initial visualization
quail_plot <- ggplot(data=quail, aes(x=Tarsus..mm., y=Culmen..mm.)) + 
  geom_point()
quail_plot


#model with least squares
quail_mod <- lm(Culmen..mm. ~ Tarsus..mm., data=quail)

#test least squares assumptions
plot(quail_mod, which=1) #no relationship between fitted and residuals
plot(quail_mod, which=2) #something a little funky at ends

hist(residuals(quail_mod)) #normally distributed


#model with likelihood
#glm
quail_glm <- glm(Culmen..mm. ~ Tarsus..mm., 
                family = gaussian(link = "identity"), 
                data = quail)

#test likelihood assumptions
fitted <- predict(quail_glm) 
res <- residuals(quail_glm) 
 
qplot(fitted, res) #no relationship between fitted and residuals

qqnorm(res) 
qqline(res) #again, a little something weird at the ends

hist(res) #normally distributed



#model with Bayes
quail_bayes <- brm(Culmen..mm. ~ Tarsus..mm.,
                     family = gaussian(link = "identity"),
                     data = quail,
                     file = "quail_bayes.brms") 


#test Bayes assumptions
plot(quail_bayes) #posterior distributions normally distributed & chains converged

#rhat
mcmc_rhat(rhat(quail_bayes)) #shows how well chains have converged: all close to 1, meaning they are the same

#autocorrelation
mcmc_acf(as.data.frame(quail_bayes)) #drop off quickly, good

#predicted vs residuals
quail_bayes_fit <- predict(quail_bayes) %>% as_tibble
quail_bayes_res <- residuals(quail_bayes)%>% as_tibble

qplot(quail_bayes_res$Estimate, quail_bayes_fit$Estimate) #no relationship

```

### 5.2 Three interpretations (10 points) 
#### OK, now that we have fits, take a look! Do the coefficients and their associated measures of error in their estimation match? How would we interpret the results from these different analyses differently? Or would we? Note, confint works on lm objects as well. 

The three different analyses resulted in the same (or very similar) estimates for coefficients and CI's. However, these results should be interpreted somewhat differently even though the outcome of those interpretations are similar (i.e. describing the relationship between Tarsus and culmen lengths). The least squares fit uses a null hypothesis (that there is no relationship between Tarsus length and culmen length) that we test with test statistics and a resulting p-value. The model fit using least squares is a frequentist approach, which means that there is theoretically one 'true' value of culmen length given a specific tarsus length. Likelihood also uses test statistics and a p-value, like the least-squares frequentist approach. However, likelihood allows for comparisons of hypotheses, not just to reject a null hypothesis. In turn, this doesn't give us one 'true' value like in least squares and can be less limiting in that respect. Fitting the data using Bayesian inference gives probabilities of possible values describing the relationship between variables, also not resulting in one 'true' value. This is a more flexible framework with which to analyze data and determine actual probabilities/predictions. 

```{r, message = FALSE, warning = FALSE}
#least squares fit

#f-tests of model
tidy(anova(quail_mod))

#t-tests of parameters
summary(quail_mod)

#CIs
confint(quail_mod)

#plot with line
quail_plot + 
  stat_smooth(method=lm, formula=y~x,
              color = "red") 


#likelihood fit
#coefficients
summary(quail_glm)

#likelihood CIs
confint(quail_glm)

#plot with line
quail_plot + 
  geom_point() + 
  stat_smooth(method = "glm", 
              method.args = list(family = gaussian(link="identity")),
              color = "red")





#bayesian fit
pp_check(quail_bayes, type="scatter") #looks like close to 1:1 fit

#normality
qqnorm(quail_bayes_fit$Estimate)
qqline(quail_bayes_res$Estimate) # a littl weird
pp_check(quail_bayes, type="error_hist") #normally distributed

##match to posterior
pp_check(quail_bayes, type="stat_2d") #good scatter
pp_check(quail_bayes, nsamples = 100) #bimodal that isn't being picked up super well...

#coefficients
summary(quail_bayes, digits=5)

#confidence intervals
posterior_interval(quail_bayes)

#visualize
quail_chains <- as.data.frame(quail_bayes)

quail_plot +
  geom_abline(intercept=quail_chains[,1], slope = quail_chains[,2], alpha=0.1, color="lightgrey") +
  geom_abline(intercept=fixef(quail_bayes)[1], slope = fixef(quail_bayes)[2], color="red") +
  geom_point()

```


### 5.3 Everyday I'm Profilin' (10 points) 
#### For your likelihood fit, are your profiles well behaved? For just the slope, use grid sampling to create a profile. You'll need to write functions for this, and use the results from your glm() fit to provide the reasonable bounds of what you should be profiling over (3SE should do). Is it well behaved? Plot the profile and give the 80% and 95% CI. Verify your results with profileModel . 

Profiles look good, but had trouble with getting the CI's for slope. (I could get them for SD though...)

```{r}
#likelihood function
likelihood_fun <- function(slope, intercept, sd){
  #data generating process
  culmen_fit <- intercept + slope * quail$Tarsus..mm.
  
  #likelihood
  sum(dnorm(quail$Culmen..mm., culmen_fit, sd, log=TRUE))
  
}

#eyeball parameters
summary(quail_glm)

#grid sampling
grid_samp <- crossing(intercept = seq(-0.05,0.05,0.01),
                      slope = seq(0.1,0.6,0.01),
                      sd = seq(1,2, 0.01)) %>%
  rowwise() %>%
  mutate(loglike = likelihood_fun(slope, intercept, sd)) %>%
  ungroup()

#Not working....not calculating likelihood
head(grid_samp)

#View(quail); NA's causing the problem?

quail2 <- na.omit(quail)


likelihood_fun <- function(slope, intercept, sd){
  #data generating process
  culmen_fit <- intercept + slope * quail2$Tarsus..mm.
  
  #likelihood
  sum(dnorm(quail2$Culmen..mm., culmen_fit, sd, log=TRUE))
  
}

#grid sampling second attempt
grid_samp2 <- crossing(intercept = seq(-0.05,0.05,0.01),
                      slope = seq(0.1,0.6,0.01),
                      sd = seq(1,2, 0.01)) %>%
  rowwise() %>%
  mutate(loglike = likelihood_fun(slope, intercept, sd)) %>%
  ungroup()

#worked!!
head(grid_samp2)


#the ML estimates are
grid_samp2 %>% filter(loglike == max(loglike))


#profiles
quail_slope_profile <- grid_samp2 %>%   
  group_by(slope) %>%   
  filter(loglike == max(loglike)) %>%   
  ungroup() 

qplot(slope, loglike, data=quail_slope_profile, geom = "line")


quail_sd_profile <- grid_samp2 %>%   
  group_by(sd) %>%   
  filter(loglike == max(loglike)) %>%   
  ungroup() 

qplot(sd, loglike, data=quail_sd_profile, geom = "line")


quail_int_profile <- grid_samp2 %>%   
  group_by(intercept) %>%   
  filter(loglike == max(loglike)) %>%   
  ungroup() 

qplot(intercept, loglike, data=quail_int_profile, geom = "line")


##From Likelihood Lab: "for each variable we want the profile of, we look at the MLE estimate at each value of the other parameter. So, group by the other parameter and filter out the MLE."

# I tried both SD and slope, but only SD seems to give me CIs...

#CIs
#95% SD CI = 1.18-1.30; slope is only returning 0.37; also tried for intercept (-0.05, 0.05; the range of the grid sampling...)
qchisq(0.95,1)/2

quail_sd_profile %>%   
  filter(loglike > max(loglike) - 1.920729) %>% 
  filter(row_number()==1 | row_number()==n())

quail_slope_profile %>%   
  filter(loglike > max(loglike) - 1.920729) %>% 
  filter(row_number()==1 | row_number()==n())


quail_int_profile %>%   
  filter(loglike > max(loglike) - 1.920729) %>% 
  filter(row_number()==1 | row_number()==n())



#80% SD CI = 1.20-1.27; slope is still only returning 0.37(??)
#I dont understand why they aren't not working :(

qchisq(0.80,1)/2

quail_sd_profile %>%   
  filter(loglike > max(loglike) - 0.8211872) %>% 
  filter(row_number()==1 | row_number()==n())

quail_slope_profile %>%   
  filter(loglike > max(loglike) - 0.8211872) %>% 
  filter(row_number()==1 | row_number()==n())

quail_int_profile %>%   
  filter(loglike > max(loglike) - 0.8211872) %>% 
  filter(row_number()==1 | row_number()==n())


#plot with CIs
colnames(quail_slope_profile)


#plotted it with a line through 0.37 since I couldn't get the CI's for some reason
ggplot(quail_slope_profile, aes(x = slope, y = loglike)) +
  geom_line() +
  geom_vline(xintercept=0.37, lty=2)


#plotted SD since I could get those...
#couldn't get a legend to show

colnames(quail_sd_profile)

ggplot(quail_sd_profile, aes(x = sd, y = loglike)) +
  geom_line() +
  geom_vline(xintercept=c(1.18, 1.30), lty=2, show.legend=TRUE) +
  geom_vline(xintercept=c(1.20, 1.27), color = "red", show.legend=TRUE)


#profileModel - looks like the max is right around 0.37 for slope and intercept plot looks similar to mine
prof <- profileModel(quail_glm, 
                     objective = "ordinaryDeviance")
plot(prof, print.grid.points = TRUE)


#with CIs - I didn't successfully get slope CI's...so hard to know if they match :( These look pretty though!
prof2 <- profileModel(quail_glm,
                     objective = "ordinaryDeviance", 
                     quantile = qchisq(0.80, 0.95))
plot(prof2, print.grid.points = TRUE)

```

### 5.4 The Power of the Prior (10 points) 
#### This data set is pretty big. After excluding NAs in the variables we're interested in, it's over 766 lines of data! Now, a lot of data can overhwelm a strong prior. But only to a point. 

#### Show first that there is enough data here that a prior for the slope with an estimate of 0.4 and a sd of 0.01 is overwhelmed by the data by demonstrating that it produces similar results to our already fit flat prior. 

```{r, warning = FALSE}
#current prior
prior_summary(quail_bayes)

quail_bayes_prior <- brm(Culmen..mm. ~ Tarsus..mm.,
                           family = gaussian(link = "identity"),
                           data = quail, 
                           prior = prior(normal(0.4, 0.01), class = b),
                           file = "quail_bayes_prior_0.4.Rds")

prior_summary(quail_bayes_prior)

#show results compared to original
  #coefficients for intercept are slightly different but others are similar. CI's are different for intercept and slope (VERY wide for slope on new model)
posterior_summary(quail_bayes)
posterior_summary(quail_bayes_prior)


#data overwhelming the prior of 0.4 [0.4 at very end of tail and range of values barely includes it]
plot(quail_bayes_prior, par = "b_Tarsus..mm.")

quail_bayes_prior_posterior <- posterior_samples(quail_bayes_prior,
                                    add_chain = TRUE)

min(quail_bayes_prior_posterior$b_Tarsus..mm.)
max(quail_bayes_prior_posterior$b_Tarsus..mm.)

#previous bayes analysis [slope values look very similar; mix-max does not include 0.40]
plot(quail_bayes, par = "b_Tarsus..mm.")

quail_bayes_posterior <- posterior_samples(quail_bayes,
                                    add_chain = TRUE)

min(quail_bayes_posterior$b_Tarsus..mm.)
max(quail_bayes_posterior$b_Tarsus..mm.)


#visualize (looks the same...)
quail_chains_prior <- as.data.frame(quail_bayes_prior)

quail_plot +
  geom_abline(intercept=quail_chains_prior[,1], slope = quail_chains_prior[,2], alpha=0.1, color="lightgrey") +
  geom_abline(intercept=fixef(quail_bayes_prior)[1], slope = fixef(quail_bayes_prior)[2], color="red") +
  geom_point()

```

#### Second, see if a very small sample size (n = 10) would at least include 0.4 in it's 95% Credible Interval. 

```{r, warning = FALSE}

#random sample of quail (n = 10)
quail_sub <- sample_n(quail, 10)

#rerun bayes model
quail_bayes_sub <- brm(Culmen..mm. ~ Tarsus..mm.,
                           family = gaussian(link = "identity"),
                           data = quail_sub, 
                           prior = prior(normal(0.4, 0.01), class = b),
                           file = "quail_bayes_sub.Rds")

#this time slope is centered around the prior value of 0.4; this shows that the data did not overwhelm the prior this time

plot(quail_bayes_sub, par = "b_Tarsus..mm.")

#min-max = 0.367-0.434
quail_bayes_sub_posterior <- posterior_samples(quail_bayes_sub,
                                    add_chain = TRUE)
min(quail_bayes_sub_posterior$b_Tarsus..mm.)
max(quail_bayes_sub_posterior$b_Tarsus..mm.)

#0.4 included in CI's
posterior_summary(quail_bayes_sub)

#visualize model [CI of fit MUCH wider]
quail_chains_sub <- as.data.frame(quail_bayes_sub)

quail_plot +
  geom_abline(intercept=quail_chains_sub[,1], slope = quail_chains_sub[,2], alpha=0.1, color="lightgrey") +
  geom_abline(intercept=fixef(quail_bayes_sub)[1], slope = fixef(quail_bayes_sub)[2], color="red") +
  geom_point()

```

#### Last, demonstrate at what sample size that 95% CL first begins to include 0.4 when we have a strong prior. How much data do we really need to overcome our prior belief? Note, it takes a long time to fit these models, so, try a strategy of spacing out the 3-4 sample sizes, and then zoom in on an interesting region. 

```{r}
#sample size of 10 with prior = 0.4; INCLUDES 0.4
#sample size of 766 with prior = 0.4; DOES NOT include 0.4
#what sample size do we start to see 0.4 included?

#random sample of quail (n = 200)
quail_sub_200 <- sample_n(quail, 200)

#rerun bayes model
quail_bayes_sub_200 <- brm(Culmen..mm. ~ Tarsus..mm.,
                           family = gaussian(link = "identity"),
                           data = quail_sub_200, 
                           prior = prior(normal(0.4, 0.01), class = b),
                           file = "quail_bayes_sub_200.Rds")

##includes 0.40
plot(quail_bayes_sub_200, par = "b_Tarsus..mm.")
posterior_summary(quail_bayes_sub_200)


#random sample of quail (n = 400)
quail_sub_400 <- sample_n(quail, 400)

#rerun bayes model
quail_bayes_sub_400 <- brm(Culmen..mm. ~ Tarsus..mm.,
                           family = gaussian(link = "identity"),
                           data = quail_sub_400, 
                           prior = prior(normal(0.4, 0.01), class = b),
                           file = "quail_bayes_sub_400.Rds")

##includes 0.40, but barely
plot(quail_bayes_sub_400, par = "b_Tarsus..mm.")
posterior_summary(quail_bayes_sub_400)



#random sample of quail (n = 500)
quail_sub_500 <- sample_n(quail, 500)

#rerun bayes model
quail_bayes_sub_500 <- brm(Culmen..mm. ~ Tarsus..mm.,
                           family = gaussian(link = "identity"),
                           data = quail_sub_500, 
                           prior = prior(normal(0.4, 0.01), class = b),
                           file = "quail_bayes_sub_500.Rds")

##0.4 NOT included in CI
plot(quail_bayes_sub_500, par = "b_Tarsus..mm.")
posterior_summary(quail_bayes_sub_500)



#random sample of quail (n = 450)
quail_sub_450 <- sample_n(quail, 450)

#rerun bayes model
quail_bayes_sub_450 <- brm(Culmen..mm. ~ Tarsus..mm.,
                           family = gaussian(link = "identity"),
                           data = quail_sub_450, 
                           prior = prior(normal(0.4, 0.01), class = b),
                           file = "quail_bayes_sub_450.Rds")

##includes 0.40
plot(quail_bayes_sub_450, par = "b_Tarsus..mm.")
posterior_summary(quail_bayes_sub_450)

#Looks like somwehere between 450 and 500 samples is where 0.40 would start to show up in CI's. Therefore, n = 500 is roughly the number of samples we would need to overwhelm the prior.
```


### 6. Extra Credit Make an election forecast
#### 1 point for the correct winner. 5 points for correctly predicting the popular vote and being within 10% (3% just for trying!). 5 points for predicting the electoral college and geting no more than 5 states wrong (3 points just for trying). 5 points for predicting the senate races getting no more than 5 states wrong (3 points just for trying). 1 extra point for each vote percentage within your 80% Confidence/Credible Interval. Ditto for the house races

Did what I could with what time I had left! From what I was able to do (if correct) it looks like the race for the house was too close to reliably call! There is a large overlap in confidence intervals. 

```{r, message = FALSE, warning = FALSE}
library(pollstR)
library(reshape2)

#used data from pollstR: An R Client for the HuffPost Pollster API

#house race data
slug <- "2018-national-house-race"
polls <- pollster_charts_polls(slug)[["content"]]
str(polls)


#generate weights & multiply each poll by weight
polls <- polls %>%
  as.data.frame() %>%
  mutate(pollWeights = polls$observations/sum(polls$observations, na.rm = T))

polls <- polls %>%
  mutate(DemocratWeighted = polls$Democrat * polls$pollWeights,
         RepublicanWeighted = polls$Republican * polls$pollWeights,
         OtherWeighted = polls$Other * polls$pollWeights,
         UndecidedWeighted = polls$Undecided * polls$pollWeights)

colnames(polls)

#sum for weighted average
sumDems <- sum(polls$DemocratWeighted, na.rm = T)
sumReps <- sum(polls$RepublicanWeighted, na.rm = T)
sumOther <- sum(polls$OtherWeighted, na.rm = T)
sumUndec <- sum(polls$UndecidedWeighted, na.rm = T)

#make sure equals ~100
sum(sumDems, sumReps, sumOther, sumUndec)

#weighted variance
Dem_Var <-  sqrt(sum(polls$pollWeights * (polls$Democrat-sumDems)^2, na.rm = T))
Rep_Var <-  sqrt(sum(polls$pollWeights * (polls$Republican-sumReps)^2, na.rm = T))
Undec_Var <-  sqrt(sum(polls$pollWeights * (polls$Undecided-sumUndec)^2, na.rm = T))
Other_Var <-  sqrt(sum(polls$pollWeights * (polls$Other-sumOther)^2, na.rm = T))

#probability density functions
polls <- polls %>%
  mutate(Dem_pdf = dnorm(polls$Democrat,sumDems, Dem_Var),
         Rep_pdf = dnorm(polls$Republican,sumReps, Rep_Var),
         Other_pdf = dnorm(polls$Other,sumOther, Other_Var),
         Undec_pdf = dnorm(polls$Undecided,sumUndec, Undec_Var))

colnames(polls)

#plot densities; normal
ggplot(polls, aes(x = Democrat, y = Dem_pdf)) +
  geom_line()

#normal
ggplot(polls, aes(x = Republican, y = Rep_pdf)) +
  geom_line()

#little funky (lack of data?); looks like gamma distribution
ggplot(polls, aes(x = Other, y = Other_pdf)) +
  geom_line()

#normal
ggplot(polls, aes(x = Undecided, y = Undec_pdf)) +
  geom_line()


#CIs
#Democrats
Dem_CI_low <- sumDems - (2 * Dem_Var)
Dem_CI_high <-sumDems + (2 * Dem_Var)
Dem_CI_low; Dem_CI_high


#Republicans
Rep_CI_low <- sumReps - (2 * Rep_Var)
Rep_CI_high <- sumReps + (2 * Rep_Var)
Rep_CI_low; Rep_CI_high

#Democrats: 35.78359 - 51.00729
#Republicans: 30.29583  - 43.69387
#so much overlap!!!


#write.csv(polls,'polls.csv')
#having trouble manipulating dataframe into usable columns for plotting variables
#made my own spreadsheet to use for plots

polls_2 <- read.csv("polls_tweaked.csv")
head(polls_2)

#boxplots (a lot of overlap)
ggplot(polls_2, aes(x = Party, y = Weighted)) +
  geom_boxplot()

#probability density function overlap
ggplot(polls_2, aes(pdf, fill=Party)) + 
  geom_density(alpha=.4, color=NA)

#weighted polls overlap
ggplot(polls_2, aes(Weighted, fill=Party)) + 
  geom_density(alpha=.4, color=NA)

#just thought these were interesting
ggplot(polls_2, aes(Percent, fill=Party)) + 
  geom_histogram(alpha=.4, color=NA)

ggplot(polls_2, aes(Percent, fill=Party)) + 
  geom_density(alpha=.4, color=NA)

```

